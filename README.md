# ğŸ§  Recursive Transformer in Transformer (RTNT)

![RTNT Architecture](rtnt.png)

This project is a PyTorch reimplementation and extension of the **Transformer in Transformer (TNT)** architecture, applied to the MNIST dataset. We extend the original idea by introducing a **recursive TNT structure** to deepen the modelâ€™s ability to capture hierarchical representations in image data.

---

## ğŸš€ Features

- ğŸ” **Recursive TNT blocks** for deeper representation learning  
- ğŸ§± Modular code structure for clean experimentation  
- ğŸ“Š Custom training + evaluation loops with learning rate schedulers  
- ğŸ“‰ Real-time loss visualization with optional smoothing  
- ğŸ§ª Easily swappable datasets, loss functions, and optimizers

---

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/yourusername/rtnt.git
cd rtnt
pip install -r requirements.txt
