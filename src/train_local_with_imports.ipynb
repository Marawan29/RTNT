{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4d2aea6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d2aea6e",
        "outputId": "f5f73804-2a05-4ce6-9d54-3bc261622b43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Marawam\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\Marawam\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from typing import List, Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "from timm.models.layers import trunc_normal_\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2c9af11a",
      "metadata": {
        "id": "2c9af11a"
      },
      "outputs": [],
      "source": [
        "# Set all seeds for reproducibility\n",
        "import random\n",
        "import os\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    # Python\n",
        "    random.seed(seed)\n",
        "    # PyTorch\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
        "    # Environment variables (for CUDA)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    # PyTorch deterministic mode (may impact performance)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "88a96876",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88a96876",
        "outputId": "a646b770-c7c7-4db3-dafd-3114368733b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Can I can use GPU now? -- False\n"
          ]
        }
      ],
      "source": [
        "print(f'Can I can use GPU now? -- {torch.cuda.is_available()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "UgFbMm0Ar1J6",
      "metadata": {
        "id": "UgFbMm0Ar1J6"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a6e284fb",
      "metadata": {
        "id": "a6e284fb"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module,\n",
        "          loss_fn: nn.modules.loss._Loss,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          train_loader: torch.utils.data.DataLoader,\n",
        "          epoch: int=0,\n",
        "          scheduler=None,\n",
        "          device=device)-> List: # Add device argument\n",
        "    # ----------- <Your code> ---------------\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "        if batch_idx % (len(train_loader) // 10) == 0:\n",
        "            print(f'Epoch {epoch}: [{batch_idx*len(images)}/{len(train_loader.dataset)}] Loss: {loss.item():.3f}')\n",
        "\n",
        "\n",
        "    assert len(train_loss) == len(train_loader)\n",
        "    return train_loss\n",
        "\n",
        "def test(model: nn.Module,\n",
        "         loss_fn: nn.modules.loss._Loss,\n",
        "         test_loader: torch.utils.data.DataLoader,\n",
        "         epoch: int=0)-> Dict:\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total_num = len(test_loader.dataset)\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            # Move data to device\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Sum up batch loss\n",
        "            test_loss += loss_fn(outputs, labels).item() * images.size(0)\n",
        "\n",
        "            # Get predictions\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Store predictions\n",
        "            predictions.append(predicted)\n",
        "\n",
        "    # Concatenate all predictions\n",
        "    all_predictions = torch.cat(predictions)\n",
        "\n",
        "    # Calculate average loss\n",
        "    test_loss /= total_num\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct / total_num\n",
        "\n",
        "    # Create test statistics dictionary\n",
        "    test_stat = {\n",
        "        \"loss\": test_loss,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"prediction\": all_predictions.cpu()  # Move predictions back to CPU\n",
        "    }\n",
        "\n",
        "    # Print log\n",
        "    print(f\"Test result on epoch {epoch}: total sample: {total_num}, Avg loss: {test_stat['loss']:.3f}, Acc: {100*test_stat['accuracy']:.3f}%\")\n",
        "\n",
        "\n",
        "    # dictionary should include loss, accuracy and prediction\n",
        "    assert \"loss\" and \"accuracy\" and \"prediction\" in test_stat.keys()\n",
        "    # \"prediction\" value should be a 1D tensor\n",
        "    assert len(test_stat[\"prediction\"]) == len(test_loader.dataset)\n",
        "    assert isinstance(test_stat[\"prediction\"], torch.Tensor)\n",
        "    return test_stat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d98044fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "id": "d98044fe",
        "outputId": "171fad03-e216-440a-a45e-017e24aaf011"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Marawam\\AppData\\Local\\Temp\\ipykernel_2104\\1860675223.py:36: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  fig.show()\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 900x900 with 9 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Download MNIST and transformation\n",
        "IMG_SIZE = 32\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# Code to display images\n",
        "batch_idx, (images, targets) = next(enumerate(train_loader)) #fix!!!!!\n",
        "fig, ax = plt.subplots(3,3,figsize = (9,9))\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        image = images[i*3+j].permute(1,2,0)\n",
        "        image = image/2 + 0.5\n",
        "        ax[i,j].imshow(image)\n",
        "        ax[i,j].set_axis_off()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b559ef47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b559ef47",
        "outputId": "bb79294a-c5c6-4cd4-d9ab-13ae9893b9d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch shape: torch.Size([32, 1, 32, 32])\n",
            "Single image shape: torch.Size([1, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "# Check the shape of the entire batch\n",
        "print(\"Batch shape:\", images.shape)\n",
        "\n",
        "# Check the shape of the first image in the batch\n",
        "print(\"Single image shape:\", images[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6930122b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from recursive_tnt import RTNT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "87a67a48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87a67a48",
        "outputId": "c80b544f-4d07-4017-9a07-df823353126a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 32, 32])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "RTNT                                     [1, 10]                   4,920\n",
              "├─PatchEmbed: 1-1                        [64, 4, 6]                --\n",
              "│    └─Unfold: 2-1                       [1, 64, 16]               --\n",
              "│    └─Conv2d: 2-2                       [64, 6, 2, 2]             12\n",
              "├─LayerNorm: 1-2                         [16, 4, 24]               48\n",
              "├─Linear: 1-3                            [16, 4, 24]               576\n",
              "├─LayerNorm: 1-4                         [16, 4, 24]               48\n",
              "├─LayerNorm: 1-5                         [1, 16, 96]               192\n",
              "├─Linear: 1-6                            [1, 16, 96]               9,216\n",
              "├─LayerNorm: 1-7                         [1, 16, 96]               192\n",
              "├─Dropout: 1-8                           [1, 17, 96]               --\n",
              "├─ModuleList: 1-9                        --                        --\n",
              "│    └─Block: 2-3                        [64, 4, 6]                --\n",
              "│    │    └─LayerNorm: 3-1               [64, 4, 6]                12\n",
              "│    │    └─Attention: 3-2               [64, 4, 6]                150\n",
              "│    │    └─LayerNorm: 3-3               [64, 4, 6]                12\n",
              "│    │    └─Mlp: 3-4                     [64, 4, 6]                318\n",
              "│    │    └─LayerNorm: 3-5               [16, 4, 24]               48\n",
              "│    │    └─Linear: 3-6                  [16, 4, 24]               576\n",
              "│    │    └─LayerNorm: 3-7               [16, 4, 24]               48\n",
              "│    │    └─LayerNorm: 3-8               [16, 4, 24]               48\n",
              "│    │    └─Attention: 3-9               [16, 4, 24]               2,328\n",
              "│    │    └─LayerNorm: 3-10              [16, 4, 24]               48\n",
              "│    │    └─Mlp: 3-11                    [16, 4, 24]               4,728\n",
              "│    │    └─LayerNorm: 3-12              [1, 16, 96]               192\n",
              "│    │    └─Linear: 3-13                 [1, 16, 96]               9,216\n",
              "│    │    └─LayerNorm: 3-14              [1, 16, 96]               192\n",
              "│    │    └─LayerNorm: 3-15              [1, 17, 96]               192\n",
              "│    │    └─Attention: 3-16              [1, 17, 96]               36,960\n",
              "│    │    └─LayerNorm: 3-17              [1, 17, 96]               192\n",
              "│    │    └─Mlp: 3-18                    [1, 17, 96]               74,208\n",
              "│    └─Block: 2-4                        [64, 4, 6]                --\n",
              "│    │    └─LayerNorm: 3-19              [64, 4, 6]                12\n",
              "│    │    └─Attention: 3-20              [64, 4, 6]                150\n",
              "│    │    └─LayerNorm: 3-21              [64, 4, 6]                12\n",
              "│    │    └─Mlp: 3-22                    [64, 4, 6]                318\n",
              "│    │    └─LayerNorm: 3-23              [16, 4, 24]               48\n",
              "│    │    └─Linear: 3-24                 [16, 4, 24]               576\n",
              "│    │    └─LayerNorm: 3-25              [16, 4, 24]               48\n",
              "│    │    └─LayerNorm: 3-26              [16, 4, 24]               48\n",
              "│    │    └─Attention: 3-27              [16, 4, 24]               2,328\n",
              "│    │    └─LayerNorm: 3-28              [16, 4, 24]               48\n",
              "│    │    └─Mlp: 3-29                    [16, 4, 24]               4,728\n",
              "│    │    └─LayerNorm: 3-30              [1, 16, 96]               192\n",
              "│    │    └─Linear: 3-31                 [1, 16, 96]               9,216\n",
              "│    │    └─LayerNorm: 3-32              [1, 16, 96]               192\n",
              "│    │    └─LayerNorm: 3-33              [1, 17, 96]               192\n",
              "│    │    └─Attention: 3-34              [1, 17, 96]               36,960\n",
              "│    │    └─LayerNorm: 3-35              [1, 17, 96]               192\n",
              "│    │    └─Mlp: 3-36                    [1, 17, 96]               74,208\n",
              "│    └─Block: 2-5                        [64, 4, 6]                --\n",
              "│    │    └─LayerNorm: 3-37              [64, 4, 6]                12\n",
              "│    │    └─Attention: 3-38              [64, 4, 6]                150\n",
              "│    │    └─LayerNorm: 3-39              [64, 4, 6]                12\n",
              "│    │    └─Mlp: 3-40                    [64, 4, 6]                318\n",
              "│    │    └─LayerNorm: 3-41              [16, 4, 24]               48\n",
              "│    │    └─Linear: 3-42                 [16, 4, 24]               576\n",
              "│    │    └─LayerNorm: 3-43              [16, 4, 24]               48\n",
              "│    │    └─LayerNorm: 3-44              [16, 4, 24]               48\n",
              "│    │    └─Attention: 3-45              [16, 4, 24]               2,328\n",
              "│    │    └─LayerNorm: 3-46              [16, 4, 24]               48\n",
              "│    │    └─Mlp: 3-47                    [16, 4, 24]               4,728\n",
              "│    │    └─LayerNorm: 3-48              [1, 16, 96]               192\n",
              "│    │    └─Linear: 3-49                 [1, 16, 96]               9,216\n",
              "│    │    └─LayerNorm: 3-50              [1, 16, 96]               192\n",
              "│    │    └─LayerNorm: 3-51              [1, 17, 96]               192\n",
              "│    │    └─Attention: 3-52              [1, 17, 96]               36,960\n",
              "│    │    └─LayerNorm: 3-53              [1, 17, 96]               192\n",
              "│    │    └─Mlp: 3-54                    [1, 17, 96]               74,208\n",
              "│    └─Block: 2-6                        [64, 4, 6]                --\n",
              "│    │    └─LayerNorm: 3-55              [64, 4, 6]                12\n",
              "│    │    └─Attention: 3-56              [64, 4, 6]                150\n",
              "│    │    └─LayerNorm: 3-57              [64, 4, 6]                12\n",
              "│    │    └─Mlp: 3-58                    [64, 4, 6]                318\n",
              "│    │    └─LayerNorm: 3-59              [16, 4, 24]               48\n",
              "│    │    └─Linear: 3-60                 [16, 4, 24]               576\n",
              "│    │    └─LayerNorm: 3-61              [16, 4, 24]               48\n",
              "│    │    └─LayerNorm: 3-62              [16, 4, 24]               48\n",
              "│    │    └─Attention: 3-63              [16, 4, 24]               2,328\n",
              "│    │    └─LayerNorm: 3-64              [16, 4, 24]               48\n",
              "│    │    └─Mlp: 3-65                    [16, 4, 24]               4,728\n",
              "│    │    └─LayerNorm: 3-66              [1, 16, 96]               192\n",
              "│    │    └─Linear: 3-67                 [1, 16, 96]               9,216\n",
              "│    │    └─LayerNorm: 3-68              [1, 16, 96]               192\n",
              "│    │    └─LayerNorm: 3-69              [1, 17, 96]               192\n",
              "│    │    └─Attention: 3-70              [1, 17, 96]               36,960\n",
              "│    │    └─LayerNorm: 3-71              [1, 17, 96]               192\n",
              "│    │    └─Mlp: 3-72                    [1, 17, 96]               74,208\n",
              "│    └─Block: 2-7                        [64, 4, 6]                --\n",
              "│    │    └─LayerNorm: 3-73              [64, 4, 6]                12\n",
              "│    │    └─Attention: 3-74              [64, 4, 6]                150\n",
              "│    │    └─LayerNorm: 3-75              [64, 4, 6]                12\n",
              "│    │    └─Mlp: 3-76                    [64, 4, 6]                318\n",
              "│    │    └─LayerNorm: 3-77              [16, 4, 24]               48\n",
              "│    │    └─Linear: 3-78                 [16, 4, 24]               576\n",
              "│    │    └─LayerNorm: 3-79              [16, 4, 24]               48\n",
              "│    │    └─LayerNorm: 3-80              [16, 4, 24]               48\n",
              "│    │    └─Attention: 3-81              [16, 4, 24]               2,328\n",
              "│    │    └─LayerNorm: 3-82              [16, 4, 24]               48\n",
              "│    │    └─Mlp: 3-83                    [16, 4, 24]               4,728\n",
              "│    │    └─LayerNorm: 3-84              [1, 16, 96]               192\n",
              "│    │    └─Linear: 3-85                 [1, 16, 96]               9,216\n",
              "│    │    └─LayerNorm: 3-86              [1, 16, 96]               192\n",
              "│    │    └─LayerNorm: 3-87              [1, 17, 96]               192\n",
              "│    │    └─Attention: 3-88              [1, 17, 96]               36,960\n",
              "│    │    └─LayerNorm: 3-89              [1, 17, 96]               192\n",
              "│    │    └─Mlp: 3-90                    [1, 17, 96]               74,208\n",
              "│    └─Block: 2-8                        [64, 4, 6]                --\n",
              "│    │    └─LayerNorm: 3-91              [64, 4, 6]                12\n",
              "│    │    └─Attention: 3-92              [64, 4, 6]                150\n",
              "│    │    └─LayerNorm: 3-93              [64, 4, 6]                12\n",
              "│    │    └─Mlp: 3-94                    [64, 4, 6]                318\n",
              "│    │    └─LayerNorm: 3-95              [16, 4, 24]               48\n",
              "│    │    └─Linear: 3-96                 [16, 4, 24]               576\n",
              "│    │    └─LayerNorm: 3-97              [16, 4, 24]               48\n",
              "│    │    └─LayerNorm: 3-98              [16, 4, 24]               48\n",
              "│    │    └─Attention: 3-99              [16, 4, 24]               2,328\n",
              "│    │    └─LayerNorm: 3-100             [16, 4, 24]               48\n",
              "│    │    └─Mlp: 3-101                   [16, 4, 24]               4,728\n",
              "│    │    └─LayerNorm: 3-102             [1, 16, 96]               192\n",
              "│    │    └─Linear: 3-103                [1, 16, 96]               9,216\n",
              "│    │    └─LayerNorm: 3-104             [1, 16, 96]               192\n",
              "│    │    └─LayerNorm: 3-105             [1, 17, 96]               192\n",
              "│    │    └─Attention: 3-106             [1, 17, 96]               36,960\n",
              "│    │    └─LayerNorm: 3-107             [1, 17, 96]               192\n",
              "│    │    └─Mlp: 3-108                   [1, 17, 96]               74,208\n",
              "│    └─Block: 2-9                        [64, 4, 6]                --\n",
              "│    │    └─LayerNorm: 3-109             [64, 4, 6]                12\n",
              "│    │    └─Attention: 3-110             [64, 4, 6]                150\n",
              "│    │    └─LayerNorm: 3-111             [64, 4, 6]                12\n",
              "│    │    └─Mlp: 3-112                   [64, 4, 6]                318\n",
              "│    │    └─LayerNorm: 3-113             [16, 4, 24]               48\n",
              "│    │    └─Linear: 3-114                [16, 4, 24]               576\n",
              "│    │    └─LayerNorm: 3-115             [16, 4, 24]               48\n",
              "│    │    └─LayerNorm: 3-116             [16, 4, 24]               48\n",
              "│    │    └─Attention: 3-117             [16, 4, 24]               2,328\n",
              "│    │    └─LayerNorm: 3-118             [16, 4, 24]               48\n",
              "│    │    └─Mlp: 3-119                   [16, 4, 24]               4,728\n",
              "│    │    └─LayerNorm: 3-120             [1, 16, 96]               192\n",
              "│    │    └─Linear: 3-121                [1, 16, 96]               9,216\n",
              "│    │    └─LayerNorm: 3-122             [1, 16, 96]               192\n",
              "│    │    └─LayerNorm: 3-123             [1, 17, 96]               192\n",
              "│    │    └─Attention: 3-124             [1, 17, 96]               36,960\n",
              "│    │    └─LayerNorm: 3-125             [1, 17, 96]               192\n",
              "│    │    └─Mlp: 3-126                   [1, 17, 96]               74,208\n",
              "│    └─Block: 2-10                       [64, 4, 6]                --\n",
              "│    │    └─LayerNorm: 3-127             [64, 4, 6]                12\n",
              "│    │    └─Attention: 3-128             [64, 4, 6]                150\n",
              "│    │    └─LayerNorm: 3-129             [64, 4, 6]                12\n",
              "│    │    └─Mlp: 3-130                   [64, 4, 6]                318\n",
              "│    │    └─LayerNorm: 3-131             [16, 4, 24]               48\n",
              "│    │    └─Linear: 3-132                [16, 4, 24]               576\n",
              "│    │    └─LayerNorm: 3-133             [16, 4, 24]               48\n",
              "│    │    └─LayerNorm: 3-134             [16, 4, 24]               48\n",
              "│    │    └─Attention: 3-135             [16, 4, 24]               2,328\n",
              "│    │    └─LayerNorm: 3-136             [16, 4, 24]               48\n",
              "│    │    └─Mlp: 3-137                   [16, 4, 24]               4,728\n",
              "│    │    └─LayerNorm: 3-138             [1, 16, 96]               192\n",
              "│    │    └─Linear: 3-139                [1, 16, 96]               9,216\n",
              "│    │    └─LayerNorm: 3-140             [1, 16, 96]               192\n",
              "│    │    └─LayerNorm: 3-141             [1, 17, 96]               192\n",
              "│    │    └─Attention: 3-142             [1, 17, 96]               36,960\n",
              "│    │    └─LayerNorm: 3-143             [1, 17, 96]               192\n",
              "│    │    └─Mlp: 3-144                   [1, 17, 96]               74,208\n",
              "│    └─Block: 2-11                       [64, 4, 6]                --\n",
              "│    │    └─LayerNorm: 3-145             [64, 4, 6]                12\n",
              "│    │    └─Attention: 3-146             [64, 4, 6]                150\n",
              "│    │    └─LayerNorm: 3-147             [64, 4, 6]                12\n",
              "│    │    └─Mlp: 3-148                   [64, 4, 6]                318\n",
              "│    │    └─LayerNorm: 3-149             [16, 4, 24]               48\n",
              "│    │    └─Linear: 3-150                [16, 4, 24]               576\n",
              "│    │    └─LayerNorm: 3-151             [16, 4, 24]               48\n",
              "│    │    └─LayerNorm: 3-152             [16, 4, 24]               48\n",
              "│    │    └─Attention: 3-153             [16, 4, 24]               2,328\n",
              "│    │    └─LayerNorm: 3-154             [16, 4, 24]               48\n",
              "│    │    └─Mlp: 3-155                   [16, 4, 24]               4,728\n",
              "│    │    └─LayerNorm: 3-156             [1, 16, 96]               192\n",
              "│    │    └─Linear: 3-157                [1, 16, 96]               9,216\n",
              "│    │    └─LayerNorm: 3-158             [1, 16, 96]               192\n",
              "│    │    └─LayerNorm: 3-159             [1, 17, 96]               192\n",
              "│    │    └─Attention: 3-160             [1, 17, 96]               36,960\n",
              "│    │    └─LayerNorm: 3-161             [1, 17, 96]               192\n",
              "│    │    └─Mlp: 3-162                   [1, 17, 96]               74,208\n",
              "│    └─Block: 2-12                       [64, 4, 6]                --\n",
              "│    │    └─LayerNorm: 3-163             [64, 4, 6]                12\n",
              "│    │    └─Attention: 3-164             [64, 4, 6]                150\n",
              "│    │    └─LayerNorm: 3-165             [64, 4, 6]                12\n",
              "│    │    └─Mlp: 3-166                   [64, 4, 6]                318\n",
              "│    │    └─LayerNorm: 3-167             [16, 4, 24]               48\n",
              "│    │    └─Linear: 3-168                [16, 4, 24]               576\n",
              "│    │    └─LayerNorm: 3-169             [16, 4, 24]               48\n",
              "│    │    └─LayerNorm: 3-170             [16, 4, 24]               48\n",
              "│    │    └─Attention: 3-171             [16, 4, 24]               2,328\n",
              "│    │    └─LayerNorm: 3-172             [16, 4, 24]               48\n",
              "│    │    └─Mlp: 3-173                   [16, 4, 24]               4,728\n",
              "│    │    └─LayerNorm: 3-174             [1, 16, 96]               192\n",
              "│    │    └─Linear: 3-175                [1, 16, 96]               9,216\n",
              "│    │    └─LayerNorm: 3-176             [1, 16, 96]               192\n",
              "│    │    └─LayerNorm: 3-177             [1, 17, 96]               192\n",
              "│    │    └─Attention: 3-178             [1, 17, 96]               36,960\n",
              "│    │    └─LayerNorm: 3-179             [1, 17, 96]               192\n",
              "│    │    └─Mlp: 3-180                   [1, 17, 96]               74,208\n",
              "│    └─Block: 2-13                       [64, 4, 6]                --\n",
              "│    │    └─LayerNorm: 3-181             [64, 4, 6]                12\n",
              "│    │    └─Attention: 3-182             [64, 4, 6]                150\n",
              "│    │    └─LayerNorm: 3-183             [64, 4, 6]                12\n",
              "│    │    └─Mlp: 3-184                   [64, 4, 6]                318\n",
              "│    │    └─LayerNorm: 3-185             [16, 4, 24]               48\n",
              "│    │    └─Linear: 3-186                [16, 4, 24]               576\n",
              "│    │    └─LayerNorm: 3-187             [16, 4, 24]               48\n",
              "│    │    └─LayerNorm: 3-188             [16, 4, 24]               48\n",
              "│    │    └─Attention: 3-189             [16, 4, 24]               2,328\n",
              "│    │    └─LayerNorm: 3-190             [16, 4, 24]               48\n",
              "│    │    └─Mlp: 3-191                   [16, 4, 24]               4,728\n",
              "│    │    └─LayerNorm: 3-192             [1, 16, 96]               192\n",
              "│    │    └─Linear: 3-193                [1, 16, 96]               9,216\n",
              "│    │    └─LayerNorm: 3-194             [1, 16, 96]               192\n",
              "│    │    └─LayerNorm: 3-195             [1, 17, 96]               192\n",
              "│    │    └─Attention: 3-196             [1, 17, 96]               36,960\n",
              "│    │    └─LayerNorm: 3-197             [1, 17, 96]               192\n",
              "│    │    └─Mlp: 3-198                   [1, 17, 96]               74,208\n",
              "│    └─Block: 2-14                       [64, 4, 6]                --\n",
              "│    │    └─LayerNorm: 3-199             [64, 4, 6]                12\n",
              "│    │    └─Attention: 3-200             [64, 4, 6]                150\n",
              "│    │    └─LayerNorm: 3-201             [64, 4, 6]                12\n",
              "│    │    └─Mlp: 3-202                   [64, 4, 6]                318\n",
              "│    │    └─LayerNorm: 3-203             [16, 4, 24]               48\n",
              "│    │    └─Linear: 3-204                [16, 4, 24]               576\n",
              "│    │    └─LayerNorm: 3-205             [16, 4, 24]               48\n",
              "│    │    └─LayerNorm: 3-206             [16, 4, 24]               48\n",
              "│    │    └─Attention: 3-207             [16, 4, 24]               2,328\n",
              "│    │    └─LayerNorm: 3-208             [16, 4, 24]               48\n",
              "│    │    └─Mlp: 3-209                   [16, 4, 24]               4,728\n",
              "│    │    └─LayerNorm: 3-210             [1, 16, 96]               192\n",
              "│    │    └─Linear: 3-211                [1, 16, 96]               9,216\n",
              "│    │    └─LayerNorm: 3-212             [1, 16, 96]               192\n",
              "│    │    └─LayerNorm: 3-213             [1, 17, 96]               192\n",
              "│    │    └─Attention: 3-214             [1, 17, 96]               36,960\n",
              "│    │    └─LayerNorm: 3-215             [1, 17, 96]               192\n",
              "│    │    └─Mlp: 3-216                   [1, 17, 96]               74,208\n",
              "├─LayerNorm: 1-10                        [1, 17, 96]               192\n",
              "├─Linear: 1-11                           [1, 10]                   970\n",
              "==========================================================================================\n",
              "Total params: 1,569,982\n",
              "Trainable params: 1,566,910\n",
              "Non-trainable params: 3,072\n",
              "Total mult-adds (Units.MEGABYTES): 3.36\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 5.95\n",
              "Params size (MB): 6.26\n",
              "Estimated Total Size (MB): 12.22\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rtnt = RTNT(num_classes=10)\n",
        "\n",
        "demo_img = torch.randn(1, 1, 32, 32).to(device)\n",
        "print(demo_img.shape)\n",
        "\n",
        "summary(model=rtnt,\n",
        "        input_size=demo_img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a3933a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a3933a9",
        "outputId": "b42e5ac9-6d62-476a-b077-58df49201b09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: [0/60000] Loss: 2.391\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Train for max_epoch epochs\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, max_epoch + \u001b[32m1\u001b[39m):\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     train_loss = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     all_train_losses.extend(train_loss)\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# Test\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, loss_fn, optimizer, train_loader, epoch, scheduler, device)\u001b[39m\n\u001b[32m     19\u001b[39m outputs = model(images)\n\u001b[32m     20\u001b[39m loss = loss_fn(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m optimizer.step()\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Marawam\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Marawam\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Marawam\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Let's first train the FC model. Below are there common hyperparameters.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "max_epoch = 5\n",
        "# ----------- <Your code> ---------------\n",
        "# Initialize FC model and move to device\n",
        "net2 = RTNT().to(device)\n",
        "optimizer = optim.Adam(net2.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.96)\n",
        "\n",
        "all_train_losses = []\n",
        "# Train for max_epoch epochs\n",
        "for epoch in range(1, max_epoch + 1):\n",
        "    # Train\n",
        "    train_loss = train(net2, criterion, optimizer, train_loader, epoch)\n",
        "    all_train_losses.extend(train_loss)\n",
        "    # Test\n",
        "    test_stat = test(net2, criterion, test_loader, epoch)\n",
        "\n",
        "    print(f'Epoch {epoch} finished with accuracy: {100*test_stat[\"accuracy\"]:.2f}%')\n",
        "\n",
        "\n",
        "# ----------- <End Your code> ---------------\n",
        "end = time.time()\n",
        "print(f'Finished Training after {end-start} s ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "107SJ7BttNbf",
      "metadata": {
        "id": "107SJ7BttNbf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For batch-wise plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(all_train_losses, label='Batch Loss')\n",
        "plt.xlabel('Batch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss per Batch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
